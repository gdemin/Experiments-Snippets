---
title: "Experiments&Snippets"
author: "Gregory Demin"
output: html_document
---

### 2014-11-08 Стандартное отклонение распределения выборочных средних (ЦПТ)

По идее, оно должно быть SD_выборки/sqrt(N_выборки) и не меняться с количеством повторов эксперимента. Эксперимент - это случайная выборка из N_sample элементов. Соответственно, мы повторяем этот эксперимент разное количество раз. 

```{r}

set.seed(20140811)

N_sample = 30
N_exp = c(10,50,100,200,500,1000,5000, 50000)

res = lapply(N_exp, function(N){
    experiments = replicate(N,mean(rnorm(N_sample)))
    c(mean = mean(experiments),sd = sd(experiments),sd_exper = 1/sqrt(N_sample))
    
    })

do.call(rbind,res) # две последние колонки должны быть примерно равны

```

Да, действительно не зависит от количества повторов. Впрочем, это было очевидно уже при написании программы. 
Заодно проверим покрытие - будет ли выборочное среднее в 95% случаях лежать в интервале +/-1.96*SD_выборки/sqrt(N_выборки).

```{r}

N_sample = 30
N_exp = c(10,50,100,200,500,1000,5000, 50000)

res = lapply(N_exp, function(N){
    experiments = replicate(N,abs(mean(rnorm(N_sample))) < 1.96/sqrt(N_sample))
    mean(experiments)
    
    })

do.call(rbind,res) # Должно быть в районе 0.95

```

Да, покрытие примерно 95%.


### 2014-11-08 Эксперименты с %>% и dplyr - интересно проверить, как он будет вести с any и if

```{r}
library(dplyr)
df = data.frame(a=1:5,b=5:1)

any(df$a<6) # должно быть TRUE

df$a<6 %>% any() # так не работает

df$a<6 %>% any # так не работает 

# оказывается проблема в приоритете операций
(df$a<6) %>% any() # так работает, правильное TRUE
(df$a<6) %>% any # так работает, правильное TRUE

# тест if - шансов мало

# (df$a<6) %>% any() %>% if() df$d=0 else df$d=1 # оно и не работает...


```

А общая идея была, чтобы можно было писать проверку в виде цепочек. 
Типа: 
df %>% check(a,1:5) %>% corr(99)  # то есть, наверное и так можно сделать, надо только функции доработать

Сложный вопрос - надо как-то получать имена переменных в номральном виде, как select в dplyr. Наверное, этот select и надо использовать. Типа, сначала запускаешь выделенное до corr, получаешь список ошибок. Потом, если запускаешь с corr, то будет исправление.
Вместо corr, наверное все-таки clean - чтобы не путать с корреляциями.
По пути надо таскать с собой атрибуты - текущая проверка, текущие переменные для условий. Хотя, может и лучше зафиксировать их раз и навсегда. Сделать в пакете константы. Надо как-то еще переменную id передавать.

Самая засада - надо бы удобный просмотрщик ошибок. И, может быть, набор алгоритмов исправления...

### 2014-12-21 Эксперименты с SVD-декомпозицией, анализом главных компонент и ковариацией

```{r}
# создаем случайную матрицуу размером Nxp

N = 100
p = 10

set.seed(20141221)
dat = matrix(rnorm(N*p),nrow = N)

weight = runif(N)
# вычитаем  центры из каждой колонки - без этого ничего не будет

dat = scale(dat,center = TRUE, scale= FALSE)
stdev = apply(dat,2,sd) # вектор стандартных отклонений

all.equal(
    sqrt(sum((weight - mean(weight))^2)/(N-1)), # N-1 тоже под корнем!
    sd(weight)
    )

all.equal(
    sum((weight - mean(weight))^2)/(N-1), # стандартное отклонение - тупо корень из вариации
    var(weight)
    )

all.equal(
    cov(dat),
    var(dat)  # а вот функция для вариации почему-то определена совсем не так, как стандартное отклонение
    )

all.equal(
    diag(cov(dat)),
    (apply(dat,2,sd))^2 
    )

all.equal(
    t(dat) %*% dat/(N-1),
    cov(dat) # ковариация считается довольно просто
    )  # cov(X) = t(X) * X / (nrow(X) - 1)  # только для центрированных переменных

all.equal(
    t(dat) %*% dat/(N-1)/(stdev %*% t(stdev)),
    cor(dat) # для корреляции нормируем ковариацию на стандартное отклонение соответствующих переменных
)

# если использовать вес, то там совсем все сложно становится...
# unbias estimation в R по умолчанию
weight_norm = weight/sum(weight) # нормализуем вес
all.equal(
    t(dat) %*% (dat*weight_norm)/(1-sum(weight_norm^2)),
    cov.wt(dat,center=FALSE, wt = weight_norm,method="unbias")$cov
    ) # очень странный делитель. Непонятно зачем там квадраты весов...

# однако, если maximum-likelihood, то более понятно. 
all.equal(
    t(dat) %*% (dat*weight)/sum(weight),
    cov.wt(dat,center=FALSE, wt = weight,method="ML")$cov
    ) # Чтобы считало, как в SPSS, надо делить на (sum(weight)-1)??

u_d_v = svd(dat)

# u - 

```
Для центрированного набора данных X из N наблюдений

1. cov(X) = X' * X /(N-1)
2. cov(X,вес = wt) = X' * (Xw) /sum(w)  # для метода максимального правдоподобия
3. Корреляция - это ковариация, нормированная на стандартные отклонения соответствующих переменных
